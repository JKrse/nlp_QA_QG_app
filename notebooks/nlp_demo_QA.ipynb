{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_demo_QA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "p32NWIj6lKzs",
        "0hMGsJ9xelT7",
        "yQkYpRXjhMcP",
        "Z5wiO9kKe1XB",
        "CfLKAfC_5GG8",
        "HJIYD9GUjTLH",
        "QTSddn2VoJU4",
        "Qk1cAoPKBoKt",
        "tMEOMQ7iiVEW"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JKrse/nlp_QA_QG_app/blob/master/notebooks/nlp_demo_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p32NWIj6lKzs",
        "colab_type": "text"
      },
      "source": [
        "# Demo for Question Answering\n",
        "\n",
        "Updated: 25th August 2020\n",
        "\n",
        "**To get started simple collapse \"Demo for Question Answering\" and press run. Next open \"Main\" and run the script.**\n",
        "\n",
        "---\n",
        "\n",
        "This is a prototype with various pre-trained NLP models for Questioning and Answering task.\n",
        "\n",
        "A pre-defined text snippet is defined in \"Text snippet - Data\", simple change input for new topic.\n",
        "\n",
        "---\n",
        "Hugging Face models can be seen here (with the name of the pre-trained model): \n",
        "https://huggingface.co/transformers/pretrained_models.html\n",
        "\n",
        "It is very easy to add extra model, simple follow the pattern in the code and add the name to 'model' and 'user2model'.\n",
        "\n",
        "The pre-trained AllenNLP models found through the Usage: \n",
        "https://demo.allennlp.org/reading-comprehension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hMGsJ9xelT7",
        "colab_type": "text"
      },
      "source": [
        "## Installation and packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQkYpRXjhMcP",
        "colab_type": "text"
      },
      "source": [
        "### Pip installations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiOIJpYuDdlX",
        "colab_type": "text"
      },
      "source": [
        "Install the following packages: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "175LPlsieDmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install allennlp_models\n",
        "!pip install allennlp\n",
        "!pip install transformers \n",
        "!pip install pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5wiO9kKe1XB",
        "colab_type": "text"
      },
      "source": [
        "### Import the libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdN1mSrfDtWN",
        "colab_type": "text"
      },
      "source": [
        "Import packages needed for running the prototype:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODSGIDvh7Opa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For AllenNLP pre-trained models implementations: \n",
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.rc\n",
        "\n",
        "# For Hugging face models implementations: \n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiBdMGdPe_2A",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfLKAfC_5GG8",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyv-Ay5J5Hos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    models = Config.models\n",
        "    input_messege = Config.input_messege\n",
        "    input_context = Config.input_context\n",
        "    \n",
        "    # Selecting the model:\n",
        "    model, model_library, tokenizer = select_model(models)\n",
        "    \n",
        "    context_input = input(input_context)\n",
        "    if context_input == \"demo\":\n",
        "      txt_doc = txt_demo\n",
        "    else:\n",
        "      txt_doc = context_input\n",
        "\n",
        "    print_setences(txt_doc)\n",
        "\n",
        "    while True: \n",
        "        question = input(input_messege)\n",
        "            \n",
        "        question.lower()\n",
        "        if question == \"quit\": \n",
        "            break\n",
        "        elif question == \"new\":\n",
        "            model, model_library, tokenizer = select_model(models)\n",
        "            continue\n",
        "        \n",
        "        if model_library == \"allennlp\":\n",
        "            answer = predict_QnA_allennlp(question, txt_doc, model)[\"best_span_str\"]\n",
        "        elif model_library == \"huggingface\":\n",
        "            answer = predict_QnA_huggingface(question, txt_doc, model, tokenizer)\n",
        "        elif model_library == \"huggingface_pipline\":\n",
        "            answer = model(question=question, context=txt_doc)[\"answer\"]\n",
        "\n",
        "        print(f\"\\nAnswer: {answer}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJIYD9GUjTLH",
        "colab_type": "text"
      },
      "source": [
        "### NLP pre-trained model [config]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcBO253OVkkn",
        "colab_type": "text"
      },
      "source": [
        "The 'Config' is a dictionary serving as a lookup further on.  The 'models' function can be seen as a config that makes it easy to access each pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSbbJNlCjXmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config: \n",
        "    models = {\"1\" : \"ELMo-BiDAF (Trained on SQuAD)\", # allennlp\n",
        "              \"2\" : \"BiDAG (Trained on SQuAD)\", # allennlp\n",
        "              \"3\" : \"Transformer QA (Trained on SQuAD)\", # allennlp\n",
        "              \"4\" : \"distilbert-base-cased-distilled-squad\", # huggingface\n",
        "              \"5\" : \"bert-large-uncased-whole-word-masking-finetuned-squad\" # huggingface\n",
        "              }\n",
        "    \n",
        "    input_messege =\"Please enter your question (write 'quit' to exit or 'new' for model selection):\\nQuestion: \"\n",
        "\n",
        "    input_context = \"Please insert the context (write 'demo' for default context)\\nContext: \"\n",
        "\n",
        "def modelsConfig(model):\n",
        "    tokenizer = []\n",
        "\n",
        "    if model == \"ELMo-BiDAF (Trained on SQuAD)\":\n",
        "        model_selected = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bidaf-elmo-model-2020.03.19.tar.gz\")\n",
        "        model_library = \"allennlp\"\n",
        "    elif model == \"BiDAG (Trained on SQuAD)\":\n",
        "        model_selected = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bidaf-model-2020.03.19.tar.gz\")\n",
        "        model_library = \"allennlp\"\n",
        "    elif model == \"Transformer QA (Trained on SQuAD)\":\n",
        "        model_selected = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/transformer-qa-2020-05-26.tar.gz\")\n",
        "        model_library = \"allennlp\"\n",
        "    elif model == \"distilbert-base-cased-distilled-squad\":\n",
        "        model_selected = pipeline(\"question-answering\", model=f\"{model}\")\n",
        "        model_library = \"huggingface_pipline\"\n",
        "    elif model == \"bert-large-uncased-whole-word-masking-finetuned-squad\":\n",
        "        model_selected = pipeline(\"question-answering\", model=f\"{model}\")\n",
        "        model_library = \"huggingface_pipline\"\n",
        "        # model_selected = BertForQuestionAnswering.from_pretrained(model)\n",
        "        # tokenizer = BertTokenizer.from_pretrained(model)\n",
        "        # model_library = \"huggingface\"\n",
        "    else:\n",
        "        raise Exception(f\"Invalid model name: {model}\")\n",
        "\n",
        "    return model_selected, model_library, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTSddn2VoJU4",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTUiJokyesgk",
        "colab_type": "text"
      },
      "source": [
        "Helper functions and wrappers to for redundant tasks and clarity of the code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqz1V8PEATnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_QnA_allennlp(question, passage, model): \n",
        "    ''' \n",
        "    Helper function for input convention used in hugging face implementation:\n",
        "        [QUESTION : ANSWER_TEXT]\n",
        "    '''\n",
        "    prediction = model.predict(passage=passage, question=question)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def read_txt(filename):\n",
        "    file = open(filename)\n",
        "    txt = file.read().replace(\"\\n\", \" \")\n",
        "    file.close()\n",
        "    return txt\n",
        "\n",
        "\n",
        "def user_input_screen(model_dict):\n",
        "    print(\"Hello! :) \\n\")\n",
        "    \n",
        "    print(\"This is the model selection: \\n\")\n",
        "    for i, val in enumerate(model_dict.values()):\n",
        "        print(f\"[{i+1}] {val}\")\n",
        "    \n",
        "    user_input = input(f\"\\nPick a model [*]: \")\n",
        "    \n",
        "    while str(user_input) not in list(model_dict):\n",
        "        print(\"\\nNot a valid model. Try again\\n\")\n",
        "        user_input = input(f\"\\nPick a model: \")\n",
        "    \n",
        "    \n",
        "    model_selected = model_dict[user_input]\n",
        "    print(\"\\nPerfect - you picked the model:\" \\\n",
        "            f\"\\n-------------\\n{model_selected}\\n-------------\\n\" \\\n",
        "            \"Just warming up - I'll be ready right away! \\n\")\n",
        "\n",
        "    return model_selected\n",
        "\n",
        "\n",
        "def select_model(model_dict):\n",
        "    '''\n",
        "    Wrapper function for 'user_input_screen' and 'models'\n",
        "    '''\n",
        "    user_model_select = user_input_screen(model_dict)\n",
        "    model, model_library, tokenizer  = modelsConfig(user_model_select)\n",
        "    \n",
        "    return model, model_library, tokenizer\n",
        "\n",
        "\n",
        "def print_setences(text):\n",
        "  setences = nltk.tokenize.sent_tokenize(text)\n",
        "  print(\"\\n ------------- \\n\")\n",
        "  print(\"Setences in your input:\")\n",
        "  for i, sent in enumerate(setences):\n",
        "    print(f\"- {sent}\")\n",
        "  print(\"\\n ------------- \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk1cAoPKBoKt",
        "colab_type": "text"
      },
      "source": [
        "### Q&A implementation for Hugging Face transformers\n",
        "***Not used. Better understanding of how the framework works. Using pipline API.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjTU3USccoc3",
        "colab_type": "text"
      },
      "source": [
        "Insert the text in which the model should look for an answer.\n",
        "\n",
        "Note models have limits for the input size of the text (e.g. BERT models has a miximum of 512 tokens)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8GszcmvWr1p",
        "colab_type": "text"
      },
      "source": [
        "Hugging Face pre-trained models with the necesarry pre-processing implemented as a function.\n",
        "This is a simple and modfied implementation (see source: https://colab.research.google.com/drive/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-#scrollTo=MVNVGN5-gI06)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B8-hWtuBk8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_QnA_huggingface(question, answer_text, model, tokenizer):\n",
        "    '''\n",
        "    Takes a `question` string and an `answer_text` string (which contains the\n",
        "    answer), and identifies the words within the `answer_text` that are the\n",
        "    answer. Prints them out.\n",
        "    '''\n",
        "    # ======== Tokenize ========\n",
        "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "    # Report how long the input sequence is.\n",
        "    # print(f'Query has {len(input_ids)} tokens.\\n') \n",
        "\n",
        "    # ======== Set Segment IDs ========\n",
        "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "    # The number of segment A tokens includes the [SEP] token istelf.\n",
        "    num_seg_a = sep_index + 1\n",
        "\n",
        "    # The remainder are segment B.\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "    # Construct the list of 0s and 1s.\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "    # There should be a segment_id for every input token.\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example question through the model.\n",
        "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Find the tokens with the highest `start` and `end` scores.\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "\n",
        "    # Get the string versions of the input tokens.\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Start with the first token.\n",
        "    answer = tokens[answer_start]\n",
        "\n",
        "    # Select the remaining answer tokens and join them with whitespace.\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        \n",
        "        # If it's a subword token, then recombine it with the previous token.\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        \n",
        "        # Otherwise, add a space then the token.\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMEOMQ7iiVEW",
        "colab_type": "text"
      },
      "source": [
        "## Text snippet - Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySJ-EC5OieK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt_demo = \"By most industry estimates, unstructured text accounts for 80% of the data available to companies. Text analytics is the process of analyzing unstructured text, extracting relevant information, and transforming it into structured information that can be leveraged in various ways. Infosys Text Analytics Platform (ITAP) enables faster processing of retail and commercial loan processing through digital customer onboarding, instant KYC and instant credit decisioning. Costs associated with paper-based trails and back office involvement are reduced through digitizing loan forms and information extraction for underwriting, and OCR led zero manual entry approach, cross reference document validation for identity validation. It enables customer servicing requests to be handled on self-servicing platforms through interaction transcript analytics. The platform enhances operational resilience by detecting anomalies in financial transactions and enables ‘Customer 3600’ by representing information as a knowledge graph. It supports product hyper-personalization by providing deeper client insights to reduce churn and increase cross-sell. It assists with advanced portfolio modeling for robo-advisors to take next-best-action and improve advisor productivity. It helps with document reviews and improves compliance checks and also supports transaction monitoring & fraud detection, review of risk models and stress testing. Infosys Text Analytics Platform offers a suite of six ready to use solutions and API-based services ranging from semantic search, skill knowledge graph, sentiment or subjectivity analysis, rule extraction from legal documents, document classification or categorization, email or chat-based automation, log comparison, automated data labeling, and much more.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RUnnK7Rhjf0",
        "colab_type": "text"
      },
      "source": [
        "### Default text\n",
        "The current text is about the ITAP: \n",
        "https://www.infosys.com/industries/financial-services/industry-offerings/text-analytics-platform.html\n",
        "\n",
        "\"By most industry estimates, unstructured text accounts for 80% of the data available to companies. Text analytics is the process of analyzing unstructured text, extracting relevant information, and transforming it into structured information that can be leveraged in various ways.\n",
        "\n",
        "Infosys Text Analytics Platform (ITAP) enables faster processing of retail and commercial loan processing through digital customer onboarding, instant KYC and instant credit decisioning. Costs associated with paper-based trails and back office involvement are reduced through digitizing loan forms and information extraction for underwriting, and OCR led zero manual entry approach, cross reference document validation for identity validation. It enables customer servicing requests to be handled on self-servicing platforms through interaction transcript analytics. The platform enhances operational resilience by detecting anomalies in financial transactions and enables ‘Customer 3600’ by representing information as a knowledge graph. It supports product hyper-personalization by providing deeper client insights to reduce churn and increase cross-sell. It assists with advanced portfolio modeling for robo-advisors to take next-best-action and improve advisor productivity. It helps with document reviews and improves compliance checks and also supports transaction monitoring & fraud detection, review of risk models and stress testing.\n",
        "\n",
        "Infosys Text Analytics Platform offers a suite of six ready to use solutions and API-based services ranging from semantic search, skill knowledge graph, sentiment or subjectivity analysis, rule extraction from legal documents, document classification or categorization, email or chat-based automation, log comparison, automated data labeling, and much more.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy-KdezBhgGp",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUdkJK9whjL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}