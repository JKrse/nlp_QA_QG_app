{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_demo_QG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SWQDW0sZm2YP",
        "0J-0FBjonX_c",
        "R7e3MJRMxnCw",
        "N9KFlcXpVY1G",
        "vAFEO0xnRRuk",
        "BzoeSheD4gZV"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyODIebEfMKEK2CEHcEtTGUX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JKrse/nlp_QA_QG_app/blob/master/nlp_demo_QG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWQDW0sZm2YP",
        "colab_type": "text"
      },
      "source": [
        "#Demo for Question Generation\n",
        "\n",
        "Updated: 25th August 2020\n",
        "\n",
        "**To get started simple collapse \"Demo for Question Generation\" and press run. Next open \"Main\" and run the script.**\n",
        "\n",
        "---\n",
        "\n",
        "The framework mimics ðŸ¤— transformers pipeline for easy inference\n",
        "\n",
        "**There are three pipeline tasks:** \n",
        "1. **question-generation: for single task question generation models**\n",
        "2. **multitask-qa-qg: for multi-task qa,qg models**\n",
        "3. **e2e-qg: for end-to-end question generation** \n",
        "\n",
        "With the option of using a model sizes \"small\" or a \"base\". All models are easily implemented but for this demo ***only the end-to-end question generation is included in this demo (both the small and base model)***. \n",
        "\n",
        "---\n",
        "This is a demo using patil-suraj work: https://github.com/patil-suraj/question_generation#answer-aware-question-generation\n",
        "\n",
        "For fine-tuning the models please look at the git repository. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J-0FBjonX_c",
        "colab_type": "text"
      },
      "source": [
        "## Pip installations\n",
        "Downloading the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PifQqSQJgpoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U transformers==3.0.0\n",
        "!pip install --user -U nltk\n",
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7e3MJRMxnCw",
        "colab_type": "text"
      },
      "source": [
        "## Cloning Question Generation git reposotory and changing directory\n",
        "We'll first need to download the question generation reposotory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmDyBP_kxaGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/patil-suraj/question_generation.git\n",
        "%cd question_generation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9KFlcXpVY1G",
        "colab_type": "text"
      },
      "source": [
        "## Importing libaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPPdz7p7VYLb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b4238f-e63b-44a9-b183-c36dd7364dec"
      },
      "source": [
        "from pipelines import pipeline\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRrryDQeQyRl",
        "colab_type": "text"
      },
      "source": [
        "## Setup: Functions & Demo script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAFEO0xnRRuk",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FST9z2fRhHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Config class\n",
        "class Config:\n",
        "  models = {\n",
        "      \"1\" : \"Question generation (without answer supervision) [small]\",\n",
        "      \"2\" : \"Question generation (without answer supervision) [base]\",\n",
        "  }\n",
        "\n",
        "  input_messege = \"Please enter your text (write 'quit' to exit,\" \\\n",
        "                \"'new' for model selection, or\" \\\n",
        "                \"'demo' for predefined sample text):\\n\" \\\n",
        "                \"Text input: \"\n",
        "\n",
        "\n",
        "## User input function\n",
        "def user_input_screen(model_dict):\n",
        "    print(\"Hello! :) \\n\")\n",
        "    \n",
        "    print(\"This is the model selection: \\n\")\n",
        "    for i, val in enumerate(model_dict.values()):\n",
        "        print(f\"[{i+1}] {val}\")\n",
        "    \n",
        "    user_input = input(f\"\\nPick a model [*]: \")\n",
        "    \n",
        "    while str(user_input) not in list(model_dict):\n",
        "        print(\"\\nNot a valid model. Try again\\n\")\n",
        "        user_input = input(f\"\\nPick a model: \")\n",
        "    \n",
        "    \n",
        "    model_selected = model_dict[user_input]\n",
        "    print(\"\\nPerfect - you picked the model:\" \\\n",
        "            f\"\\n-------------\\n{model_selected}\\n-------------\\n\" \\\n",
        "            \"Just warming up - I'll be ready right away! \\n\")\n",
        "\n",
        "    return model_selected\n",
        "\n",
        "\n",
        "## Load model\n",
        "def modelsConfig(model):\n",
        "  if model == \"Question generation (without answer supervision) [small]\":\n",
        "    nlp = pipeline(\"e2e-qg\", model=\"valhalla/t5-small-e2e-qg\")\n",
        "    model_library = \"e2e\"\n",
        "  elif model == \"Question generation (without answer supervision) [base]\":\n",
        "    nlp = pipeline(\"e2e-qg\", model=\"valhalla/t5-base-e2e-qg\")\n",
        "    model_library = \"e2e\"\n",
        "  else:\n",
        "    raise Exception(\"Not a valid model\")    \n",
        "  \n",
        "  return nlp, model_library\n",
        "\n",
        "def select_model(model_dict):\n",
        "    '''\n",
        "    Wrapper function for 'user_input_screen' and 'models'\n",
        "    '''\n",
        "    user_model_select = user_input_screen(model_dict)\n",
        "    nlp_model, model_library = modelsConfig(user_model_select)\n",
        "    \n",
        "    return nlp_model, model_library\n",
        "\n",
        "\n",
        "## Helper function printing sentences and questions\n",
        "def print_setences(text):\n",
        "  setences = nltk.tokenize.sent_tokenize(text)\n",
        "  print(\"\\n ------------- \\n\")\n",
        "  print(\"Setences in your input:\")\n",
        "  for i, sent in enumerate(setences):\n",
        "    print(f\"- {sent}\")\n",
        "  print(\"\\n ------------- \\n\")\n",
        "\n",
        "def print_questions_generated(questions):\n",
        "  print(\"\\n ------------- \\n\")\n",
        "  print(f\"Question Generated:\")\n",
        "  for i, question in enumerate(questions):\n",
        "    print(f\"{i+1}) {question}\")\n",
        "  print(\"\\n ------------- \\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzoeSheD4gZV",
        "colab_type": "text"
      },
      "source": [
        "## Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in9SNSYW4h9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    models_dict = Config.models\n",
        "    input_messege = Config.input_messege\n",
        "    \n",
        "    nlp_model, model_library = select_model(models_dict)\n",
        "\n",
        "    while True: \n",
        "    \n",
        "        text = str(input(input_messege))\n",
        "\n",
        "        if text == \"quit\": \n",
        "            break\n",
        "        elif text == \"new\":\n",
        "            nlp_model, model_library = select_model(models_dict)\n",
        "            continue\n",
        "        elif text == \"demo\":\n",
        "          text = demo_text\n",
        "        \n",
        "        print_setences(text)\n",
        "        text.lower()\n",
        "\n",
        "        if model_library == \"e2e\":\n",
        "            questions = nlp_model(text)    \n",
        "            print_questions_generated(questions) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEeuntpDRct5",
        "colab_type": "text"
      },
      "source": [
        "## Demo setup\n",
        "Demo setup if user does not have any text input \n",
        "\n",
        "\"Infosys Limited, is an Indian multinational corporation that provides business consulting, information technology and outsourcing services. The company is headquartered in Bangalore, Karnataka, India. Infosys is the second-largest Indian IT company after Tata Consultancy Services by 2017 revenue figures and the 596th largest public company in the world based on revenue. On 29 March 2019, its market capitalisation was $46.52 billion.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR8qzN1ARa9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "demo_text = \"Infosys Limited, is an Indian multinational corporation that provides business consulting, information technology and outsourcing services. The company is headquartered in Bangalore, Karnataka, India. Infosys is the second-largest Indian IT company after Tata Consultancy Services by 2017 revenue figures and the 596th largest public company in the world based on revenue. On 29 March 2019, its market capitalisation was $46.52 billion.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrQXGIEpSA6_",
        "colab_type": "text"
      },
      "source": [
        "# Main\n",
        "With all dependencies install simple press run!\n",
        "\n",
        "You will have the option of picking either the small or base model for end-to-end question generation. \n",
        "Next simple provide a text of your own or write *demo* (using a predefined text snippet). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMJQqy3cjcXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}