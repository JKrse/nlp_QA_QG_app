{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_QG_QA_demo_streamlit.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UziPYW_xiiPW",
        "o96ARPBqndDL",
        "BYZ4fPN_nYXq",
        "usv0XQOsItHp",
        "W1TWpgyqsFHa",
        "Vy2WWJYjF6Wx"
      ],
      "authorship_tag": "ABX9TyMivKsWrRZCUv5gCwH0YQG/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JKrse/nlp_streamlit_QG_QA/blob/master/nlp_QG_QA_demo_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UziPYW_xiiPW",
        "colab_type": "text"
      },
      "source": [
        "# Streamlit demo for Question Answering & Question Generation\n",
        "\n",
        "Updated: 1st September 2020\n",
        "\n",
        "---\n",
        "\n",
        "This notebook enables you to run a streamlit app through a COLAB. As the demo includes a number of models used for Question Answering and Question Generation it can be somewhat storage and computational demanding, which you don't have to think about here!\n",
        "\n",
        "The final streamlit is essentially a combination of two seperate COLAB prototypes solving Question Answering and Question Generation respectively. These are now put together in a nice streamlit app with user interface. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o96ARPBqndDL",
        "colab_type": "text"
      },
      "source": [
        "## **Question Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tuagnTlnLoW",
        "colab_type": "text"
      },
      "source": [
        "- https://colab.research.google.com/drive/1D5T4Y2AZapoeNeD9fNcX0hqNQ36Vg7Vp?usp=sharing\n",
        "\n",
        "**To get started simple collapse \"Demo for Question Generation\" and press run. Next open \"Main\" and run the script.**\n",
        "\n",
        "---\n",
        "\n",
        "The framework mimics ðŸ¤— transformers pipeline for easy inference\n",
        "\n",
        "**There are three pipeline tasks:** \n",
        "1. **question-generation: for single task question generation models**\n",
        "2. **multitask-qa-qg: for multi-task qa,qg models**\n",
        "3. **e2e-qg: for end-to-end question generation** \n",
        "\n",
        "With the option of using a model sizes \"small\" or a \"base\". All models are easily implemented but for this demo ***only the end-to-end question generation is included in this demo (both the small and base model)***. \n",
        "\n",
        "---\n",
        "This is a demo using patil-suraj work: https://github.com/patil-suraj/question_generation\n",
        "\n",
        "For fine-tuning the models please look at the git repository. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZ4fPN_nYXq",
        "colab_type": "text"
      },
      "source": [
        "##**Question Answering**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbaTzsOfnRL8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- https://colab.research.google.com/drive/1s6WH45ZNSyM38FDcpqYfLXejDUDKTn5s?usp=sharing\n",
        "\n",
        "This is a prototype with various pre-trained NLP models for Questioning and Answering task.\n",
        "\n",
        "A pre-defined text snippet is defined in \"Text snippet - Data\", simple change input for new topic.\n",
        "\n",
        "Hugging Face models can be seen here (with the name of the pre-trained model): https://huggingface.co/transformers/pretrained_models.html\n",
        "\n",
        "It is very easy to add extra model, simple follow the pattern in the code and add the name to 'model' and 'user2model'.\n",
        "\n",
        "The pre-trained AllenNLP models found through the Usage: https://demo.allennlp.org/reading-comprehension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sACGfYp_E9AP",
        "colab_type": "text"
      },
      "source": [
        "# Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usv0XQOsItHp",
        "colab_type": "text"
      },
      "source": [
        "## Installing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGae1Lf9m7yT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install gitpython\n",
        "!pip install nltk\n",
        "\n",
        "!pip install allennlp==1.0.0\n",
        "!pip install allennlp_models\n",
        "\n",
        "!pip install -U transformers==3.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1TWpgyqsFHa",
        "colab_type": "text"
      },
      "source": [
        "## Clone Git\n",
        "The git for Question Generation is cloned for this COLAB. Also, the app itself is cloned. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98vcCWAysOeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import git\n",
        "if not os.path.exists(f\"{os.getcwd()}/question_generation/\"):\n",
        "    git.Git(os.getcwd()).clone(\"https://github.com/patil-suraj/question_generation.git\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Oa01IgE1l3",
        "colab_type": "text"
      },
      "source": [
        "# Write App scripts\n",
        "Writing the python scripts used when running the app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jfir_zoGPRc",
        "colab_type": "text"
      },
      "source": [
        "### App"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gokXHP1NE028",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4bcac46-312b-4f21-e8d8-8d6c18506af8"
      },
      "source": [
        "%%writefile app.py\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "import streamlit as st\n",
        "from screens import *\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "PAGE_CONFIG = {\"page_title\":\"QG_QA_demo.io\",\"page_icon\":\":shark:\",\"layout\":\"centered\"}\n",
        "st.beta_set_page_config(**PAGE_CONFIG)\n",
        "\n",
        "# App start: \n",
        "def main():\n",
        "    menu = [\"About\", \"Question Answering\", \"Question Generation\"]\n",
        "    choice = st.sidebar.selectbox(\"Menu\", menu)\n",
        "    \n",
        "    #===========================================================================\n",
        "    # Main Page: \n",
        "    if choice == \"About\":\n",
        "        main_screen()\n",
        "\n",
        "    #===========================================================================\n",
        "    # QA Page: \n",
        "    if choice == \"Question Answering\": \n",
        "        QA_screen()\n",
        "\n",
        "    #===========================================================================\n",
        "    # QG Page: \n",
        "    if choice == \"Question Generation\":\n",
        "        QG_screen()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xdck__nF_FV",
        "colab_type": "text"
      },
      "source": [
        "### Screens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZh3-TWzF-sX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23dc2e40-db75-45e7-d892-0f2d087b496a"
      },
      "source": [
        "%%writefile screens.py\n",
        "\n",
        "#===============================================================================\n",
        "import streamlit as st\n",
        "from functions import *\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import numpy as np\n",
        "#===============================================================================\n",
        "\n",
        "# Input variables  \n",
        "models_dict_qg = Config.models_qg\n",
        "models_dict_qa = Config.models_qa\n",
        "\n",
        "demo_text_qg = Config.demo_text_qg\n",
        "demo_text_qa = Config.demo_text_qa[\"context\"]\n",
        "demo_ques_qa = Config.demo_text_qa[\"question\"]\n",
        "\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "def QG_screen():\n",
        "    st.title(\"Question Generation\")\t\n",
        "    st.write(\"Question Generation (QG) aims to generate natural language, \"\\\n",
        "            \"questions based on given contents where the generated questions \"\\\n",
        "            \"need to be able to be answered by the contents.\")\n",
        "    #####################################\n",
        "    st.subheader(\"Model selection\")\n",
        "    #### Model select and user input ####\n",
        "    option_qg = st.selectbox(\"Select model size:\",\n",
        "                        (list(models_dict_qg.keys())))\n",
        "    st.subheader(\"Provide context\")\n",
        "    user_input_qg = st.text_area(\"Please provide context text:\", height=200,\n",
        "                                value=f\"{demo_text_qg}\", max_chars=500)\n",
        "    \n",
        "    ######################################\n",
        "    # List user context setence by setence\n",
        "    sentences_qg = sent_tokenize(user_input_qg)\n",
        "    list_context(\"Sentences\", sentences_qg)\n",
        "\n",
        "    ######################################\n",
        "    #### Load the NLP model: ####\n",
        "    nlp_qg  = modelsConfig_qg(option_qg)\n",
        "    questions = nlp_qg(user_input_qg)\n",
        "    \n",
        "    st.write(\"**Question Generated:**\")\n",
        "    for i, question in enumerate(questions):\n",
        "        st.write(f\"{i+1}. {question}\")\n",
        "\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "def QA_screen():\n",
        "    st.title(\"Question Answering\")\n",
        "    st.write(\"Reading comprehension is the task of answering questions about \"\\\n",
        "        \"a passage of text to show that the system understands the passage.\")\n",
        "    #####################################\n",
        "    #### Model select and user input ####\n",
        "    st.subheader(\"Model selection\")\n",
        "    option_qa = st.selectbox(\"Select model:\",\n",
        "        (list(models_dict_qa.keys())))\n",
        "\n",
        "    st.subheader(\"Provide context:\")\n",
        "    user_context_qa = st.text_area(\"Please provide text:\", height=100,\n",
        "                                value=f\"{demo_text_qa}\", max_chars=500)\n",
        "    \n",
        "    #####################################\n",
        "    #### Context setence by setence ####\n",
        "    sentences_qa = sent_tokenize(user_context_qa)\n",
        "    list_context(\"Sentences\", sentences_qa, checkbox=True)\n",
        "        \n",
        "    st.subheader(\"Provide the question(s):\")\n",
        "    user_question_qa = st.text_area(\"Please provide question text:\", height=50,\n",
        "                                value=f\"{demo_ques_qa}\", max_chars=200)\n",
        "    \n",
        "    questions = sent_tokenize(user_question_qa)\n",
        "\n",
        "    #####################################\n",
        "    #### Load the NLP model ####\n",
        "    nlp_qa = modelsConfig_qa(option_qa)\n",
        "    \n",
        "    answers = {}\n",
        "    for i, question in enumerate(questions):\n",
        "        st.write(f\"{i+1}. **Question**: {question}\")\n",
        "        answer = qa_compute_answer(nlp_qa, questions[i], user_context_qa, models_dict_qa[option_qa])\n",
        "        \n",
        "        answers[answer] = answer_index(answer, user_context_qa)\n",
        "        st.write(f\"{i+1}. **Answer**: {answer}\")\n",
        "        \n",
        "    \n",
        "    #####################################\n",
        "    #### Highlight answer in context ####\n",
        "    st.subheader(\"Answer shown in context\")\n",
        "    num_answer = range(1, len(answers)+1)\n",
        "    \n",
        "    if len(num_answer) == 1: \n",
        "        index_range = answers[list(answers)[0]]\n",
        "        write_answer(user_context_qa, index_range)\n",
        "    \n",
        "    elif len(num_answer) > 1:\n",
        "        option_qa = st.selectbox(\"Select question in context:\", list(num_answer))\n",
        "        index_range = answers[list(answers)[option_qa-1]]\n",
        "        write_answer(user_context_qa, index_range)\n",
        "\n",
        "\n",
        "    #####################################\n",
        "    \n",
        "    \n",
        "#===============================================================================\n",
        "\n",
        "def main_screen():\n",
        "    st.title(\"Question Answering & Question Generation\")\n",
        "    #####################################\n",
        "    st.subheader(\"About :trophy:\")\n",
        "    st.write(\"Hello there and welcome!\")\n",
        "    st.write(\"In this prototype you are able to play around with state-of-the-art \"\\\n",
        "            \"NLP models in an easy user friendly environment.\")\n",
        "    st.write(\"There are two main task, namely; Question Answering and Question Generation.\"\\\n",
        "        \" Use the sidebar to navigate to them respectively.\")\n",
        "    \n",
        "    #####################################\n",
        "    st.subheader(\"Github :zap:\")\n",
        "    st.write(\"You can find the git repository for the app here:\")\n",
        "    st.write(\"https://github.com/JKrse/nlp_streamlit_QG_QA\")\n",
        "\n",
        "    #####################################\n",
        "    st.subheader(\"COLAB :crocodile:\")\n",
        "    st.write(\"To play around with all of models can be both demanding in storage\"\\\n",
        "            \"(~6gb) and can be computationally. To cope with this a COLAB notebook has\"\\\n",
        "            \"been developed, which enables you to run the streamlit app through COLAB.\"\\\n",
        "            \"Models are now stored and computationen made on using Google service.\")\n",
        "    st.write(\"https://colab.research.google.com/drive/1zjWn1OEvL_OJxQufjCnrtIIq25qT9DMz?usp=sharing\")\n",
        "    st.write(\"Note you will need to modify the script a little bit by adding\"\\\n",
        "            \"your own ngrok to generate the localhost server.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing screens.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy2WWJYjF6Wx",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei2UoRGdTX0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84ef6323-eec2-434a-94e7-06a17337a985"
      },
      "source": [
        "%%writefile functions.py\n",
        "\n",
        "#===============================================================================\n",
        "import streamlit as st\n",
        "\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.rc\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "import git\n",
        "import os\n",
        "if not os.path.exists(f\"{os.getcwd()}/question_generation/\"):\n",
        "    git.Git(os.getcwd()).clone(\"https://github.com/patil-suraj/question_generation.git\")\n",
        "\n",
        "from question_generation.pipelines import pipeline as qg_pipline\n",
        "from transformers import pipeline as qa_pipline\n",
        "\n",
        "#===============================================================================\n",
        "class Config:\n",
        "    models_qg = {\n",
        "        \"Question generation (without answer supervision) [small]\" : \"qg\",\n",
        "        \"Question generation (without answer supervision) [base]\" : \"qg\",\n",
        "    }\n",
        "    \n",
        "    models_qa = {\n",
        "        \"ELMo-BiDAF (Trained on SQuAD)\" : \"allennlp\",\n",
        "        \"BiDAG (Trained on SQuAD)\" : \"allennlp\",\n",
        "        # \"Transformer QA (Trained on SQuAD)\" : \"allennlp\", # not working [hack]\n",
        "        \"distilbert-base-cased-distilled-squad\" : \"huggingface_pipline\", \n",
        "        \"bert-large-uncased-whole-word-masking-finetuned-squad\"  : \"huggingface_pipline\" \n",
        "        }\n",
        "\n",
        "\n",
        "    demo_text_qg = \"Infosys Limited, is an Indian multinational corporation\" \\\n",
        "        \"that provides business consulting, information technology\" \\\n",
        "        \"and outsourcing services. The company is headquartered in\" \\\n",
        "        \"Bangalore, Karnataka, India. Infosys is the second-largest\" \\\n",
        "        \"Indian IT company after Tata Consultancy Services by 2017 revenue\" \\\n",
        "        \"figures and the 596th largest public company in the world based\" \\\n",
        "        \"on revenue. On 29 March 2019, its market capitalisation was $46.52 billion.\"\n",
        "\n",
        "\n",
        "    demo_text_qa = {\"context\" : \"Python is a programming language. Created by Guido van Rossum and first released in 1991.\",\n",
        "                    \"question\" : \"Who created Python? When was Python first released?\"}\n",
        "\n",
        "def list_context(title, list_input, checkbox = False):\n",
        "    \n",
        "    if checkbox:\n",
        "        checkbox_sent = st.checkbox(f\"Show {str(title).lower()}\")\n",
        "        \n",
        "        if checkbox_sent:\n",
        "            st.write(f\"**{title}:**\")\n",
        "            for sent in list_input:\n",
        "                st.write(f\"- {sent}\")\n",
        "    else: \n",
        "        st.write(f\"**{title}:**\")\n",
        "        for sent in list_input:\n",
        "            st.write(f\"- {sent}\")\n",
        "\n",
        "\n",
        "## Load model\n",
        "@st.cache\n",
        "def modelsConfig_qg(model):\n",
        "    ## Question Generation: \n",
        "    if model == \"Question generation (without answer supervision) [small]\":\n",
        "        model_selected = qg_pipline(\"e2e-qg\", model=\"valhalla/t5-small-e2e-qg\")\n",
        "    \n",
        "    elif model == \"Question generation (without answer supervision) [base]\":\n",
        "        model_selected = qg_pipline(\"e2e-qg\", model=\"valhalla/t5-base-e2e-qg\")    \n",
        "    \n",
        "    else:\n",
        "        raise Exception(\"Not a valid model\")   \n",
        "\n",
        "    return model_selected\n",
        "\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def modelsConfig_qa(model):\n",
        "    ## Question Answering: \n",
        "    if model == \"ELMo-BiDAF (Trained on SQuAD)\":\n",
        "        model_selected = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bidaf-elmo-model-2020.03.19.tar.gz\")\n",
        "    elif model == \"BiDAG (Trained on SQuAD)\":\n",
        "        model_selected = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bidaf-model-2020.03.19.tar.gz\")\n",
        "    elif model == \"Transformer QA (Trained on SQuAD)\":\n",
        "        model_selected = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/transformer-qa-2020-05-26.tar.gz\")\n",
        "    elif model == \"distilbert-base-cased-distilled-squad\":\n",
        "        model_selected = qa_pipline(\"question-answering\", model=f\"{model}\")\n",
        "    elif model == \"bert-large-uncased-whole-word-masking-finetuned-squad\":\n",
        "        model_selected = qa_pipline(\"question-answering\", model=f\"{model}\")\n",
        "    else:\n",
        "        raise Exception(\"Not a valid model\")    \n",
        "    return model_selected\n",
        "\n",
        "\n",
        "\n",
        "def qa_compute_answer(model, question, context, model_library):\n",
        "    if model_library == \"allennlp\":\n",
        "        answer = predict_QnA_allennlp(question, context, model)[\"best_span_str\"]\n",
        "    \n",
        "    elif model_library == \"huggingface_pipline\":\n",
        "        answer = model(question=question, context=context)[\"answer\"]\n",
        "    return answer\n",
        "\n",
        "\n",
        "def predict_QnA_allennlp(question, passage, model): \n",
        "    ''' \n",
        "    Helper function for input convention used in hugging face implementation:\n",
        "        [QUESTION : ANSWER_TEXT]\n",
        "    '''\n",
        "    prediction = model.predict(passage=passage, question=question)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def answer_index(answer, context):\n",
        "    index_range = []\n",
        "    word_len = []    \n",
        "\n",
        "    for word in answer.split():\n",
        "        word.lower()\n",
        "        idx = context.find(word)\n",
        "        \n",
        "        word_len.append(len(word))    \n",
        "        index_range.append(idx)\n",
        "\n",
        "    index_range[-1]+word_len[-1]\n",
        "\n",
        "    answer_span = [index_range[0], index_range[-1]+word_len[-1]]\n",
        "\n",
        "    return answer_span\n",
        "\n",
        "\n",
        "def write_answer(context, answer_span):\n",
        "    st.write(f\"{context[0: answer_span[0]]}\"\\\n",
        "            f\"**{context[answer_span[0]: answer_span[1]]}**\"\n",
        "            f\"{context[answer_span[1]:]}\")    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing functions_old.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T9Sy0oSFxtp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3e77be2-94bf-4b17-9d56-c7fbbfa1342d"
      },
      "source": [
        "  %%writefile functions.py\n",
        "\n",
        "  #===============================================================================\n",
        "import streamlit as st\n",
        "\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.rc\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from question_generation.pipelines import pipeline as qg_pipline\n",
        "from transformers import pipeline as qa_pipline\n",
        "\n",
        "#===============================================================================\n",
        "class Config:\n",
        "    models_qg = {\n",
        "        \"Question generation (without answer supervision) [small]\" : \"qg\",\n",
        "        \"Question generation (without answer supervision) [base]\" : \"qg\",\n",
        "    }\n",
        "    \n",
        "    models_qa = {\n",
        "        \"ELMo-BiDAF (Trained on SQuAD)\" : \"allennlp\",\n",
        "        \"BiDAG (Trained on SQuAD)\" : \"allennlp\",\n",
        "        # \"Transformer QA (Trained on SQuAD)\" : \"allennlp\", # not working [hack]\n",
        "        \"distilbert-base-cased-distilled-squad\" : \"huggingface_pipline\", \n",
        "        \"bert-large-uncased-whole-word-masking-finetuned-squad\"  : \"huggingface_pipline\" \n",
        "        }\n",
        "\n",
        "\n",
        "    demo_text_qg = \"Infosys Limited, is an Indian multinational corporation\" \\\n",
        "        \"that provides business consulting, information technology\" \\\n",
        "        \"and outsourcing services. The company is headquartered in\" \\\n",
        "        \"Bangalore, Karnataka, India. Infosys is the second-largest\" \\\n",
        "        \"Indian IT company after Tata Consultancy Services by 2017 revenue\" \\\n",
        "        \"figures and the 596th largest public company in the world based\" \\\n",
        "        \"on revenue. On 29 March 2019, its market capitalisation was $46.52 billion.\"\n",
        "\n",
        "\n",
        "    demo_text_qa = {\"context\" : \"Python is a programming language. Created by Guido van Rossum and first released in 1991.\",\n",
        "                    \"question\" : \"Who created Python? When was Python first released?\"}\n",
        "\n",
        "def list_context(title, list_input, checkbox = False):\n",
        "    \n",
        "    if checkbox:\n",
        "        checkbox_sent = st.checkbox(f\"Show {str(title).lower()}\")\n",
        "        \n",
        "        if checkbox_sent:\n",
        "            st.write(f\"**{title}:**\")\n",
        "            for sent in list_input:\n",
        "                st.write(f\"- {sent}\")\n",
        "    else: \n",
        "        st.write(f\"**{title}:**\")\n",
        "        for sent in list_input:\n",
        "            st.write(f\"- {sent}\")\n",
        "\n",
        "\n",
        "## Load model\n",
        "@st.cache\n",
        "def modelsConfig_qg(model):\n",
        "    ## Question Generation: \n",
        "    if model == \"Question generation (without answer supervision) [small]\":\n",
        "        model_selected = qg_pipline(\"e2e-qg\", model=\"valhalla/t5-small-e2e-qg\")\n",
        "    \n",
        "    elif model == \"Question generation (without answer supervision) [base]\":\n",
        "        model_selected = qg_pipline(\"e2e-qg\", model=\"valhalla/t5-base-e2e-qg\")    \n",
        "    \n",
        "    else:\n",
        "        raise Exception(\"Not a valid model\")   \n",
        "\n",
        "    return model_selected\n",
        "\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def modelsConfig_qa(model):\n",
        "    ## Question Answering: \n",
        "    if model == \"ELMo-BiDAF (Trained on SQuAD)\":\n",
        "        model_selected = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bidaf-elmo-model-2020.03.19.tar.gz\")\n",
        "    elif model == \"BiDAG (Trained on SQuAD)\":\n",
        "        model_selected = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bidaf-model-2020.03.19.tar.gz\")\n",
        "    elif model == \"Transformer QA (Trained on SQuAD)\":\n",
        "        model_selected = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/transformer-qa-2020-05-26.tar.gz\")\n",
        "    elif model == \"distilbert-base-cased-distilled-squad\":\n",
        "        model_selected = qa_pipline(\"question-answering\", model=f\"{model}\")\n",
        "    elif model == \"bert-large-uncased-whole-word-masking-finetuned-squad\":\n",
        "        model_selected = qa_pipline(\"question-answering\", model=f\"{model}\")\n",
        "    else:\n",
        "        raise Exception(\"Not a valid model\")    \n",
        "    return model_selected\n",
        "\n",
        "\n",
        "\n",
        "def qa_compute_answer(model, question, context, model_library):\n",
        "    if model_library == \"allennlp\":\n",
        "        answer = predict_QnA_allennlp(question, context, model)[\"best_span_str\"]\n",
        "    \n",
        "    elif model_library == \"huggingface_pipline\":\n",
        "        answer = model(question=question, context=context)[\"answer\"]\n",
        "    return answer\n",
        "\n",
        "\n",
        "def predict_QnA_allennlp(question, passage, model): \n",
        "    ''' \n",
        "    Helper function for input convention used in hugging face implementation:\n",
        "        [QUESTION : ANSWER_TEXT]\n",
        "    '''\n",
        "    prediction = model.predict(passage=passage, question=question)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def answer_index(answer, context):\n",
        "    index_range = []\n",
        "    word_len = []    \n",
        "\n",
        "    for word in answer.split():\n",
        "        word.lower()\n",
        "        idx = context.find(word)\n",
        "        \n",
        "        word_len.append(len(word))    \n",
        "        index_range.append(idx)\n",
        "\n",
        "    index_range[-1]+word_len[-1]\n",
        "\n",
        "    answer_span = [index_range[0], index_range[-1]+word_len[-1]]\n",
        "\n",
        "    return answer_span\n",
        "\n",
        "\n",
        "def write_answer(context, answer_span):\n",
        "    st.write(f\"{context[0: answer_span[0]]}\"\\\n",
        "            f\"**{context[answer_span[0]: answer_span[1]]}**\"\n",
        "            f\"{context[answer_span[1]:]}\")    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing functions.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2uVNcqePuI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOorJl4gJd0M",
        "colab_type": "text"
      },
      "source": [
        "# Connecting Streamlit app from COLAB\n",
        "Following this guide for connection: https://blog.jcharistech.com/2020/08/16/how-to-run-streamlit-apps-from-googles-colab/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WbeZmdC2FbU",
        "colab_type": "text"
      },
      "source": [
        "## Your **ngrok** input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT_txREA2dC6",
        "colab_type": "text"
      },
      "source": [
        "## Get Your Authentication Tokens\n",
        "First task is to signup to ngrok.com and create a free account. This will give you access to several features.\n",
        "\n",
        "To use ngrok,you will need to an authentication token which can be found on your dashboard of your ngrok account (https://dashboard.ngrok.com/get-started/setup).\n",
        "\n",
        "This is what you will use to authenticate when working with ngrok. You can find your authtokens below the â€˜Connect Your Accountâ€™ like this\n",
        "\n",
        "./ngrok authtokens xxxxxxxxxxxxxxxxxxxx\n",
        "\n",
        "This is what you will use to connect your account."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC-J4w2g-Rxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ngrok authtoken 1gfpORjhIoMNCFJUns3Dl6g6DNt_4EduvxRAdFGyZkUmWmS6z\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHHUtikn2feq",
        "colab_type": "text"
      },
      "source": [
        "##  Generate the proxy server!\n",
        "You are ready "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpLLxGzP-ucp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e6b3d45-36af-4a8e-cca5-87278552735b"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&\n",
        "from pyngrok import ngrok\n",
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(port='8501')\n",
        "\n",
        "print(public_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://ec99a0560058.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3kXBr62-1Bm",
        "colab_type": "text"
      },
      "source": [
        "# Get your local server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb4vlzlS-_tI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(public_url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}